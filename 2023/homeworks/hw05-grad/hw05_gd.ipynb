{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f46lxLSsBp0f"
   },
   "source": [
    "# Домашнее задание 5. Градиентный спуск. (10 баллов + 2.5 бонус)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhdyseeXBp0k"
   },
   "source": [
    "В этом домашнем задании вы напишете градиентный спуск для линейной регрессии, а так же посмотрите, как он ведёт себя с разными параметрами и разными функциями потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "do1I2ygaBp0k"
   },
   "source": [
    "Правила:\n",
    "\n",
    "* Домашнее задание оценивается в 10 баллов.\n",
    "\n",
    "* Можно использовать без доказательства любые результаты, встречавшиеся на лекциях или семинарах по курсу, если получение этих результатов не является вопросом задания.\n",
    "\n",
    "* Можно использовать любые свободные источники с *обязательным* указанием ссылки на них.\n",
    "\n",
    "* Плагиат не допускается. При обнаружении случаев списывания, 0 за работу выставляется всем участникам нарушения, даже если можно установить, кто у кого списал.\n",
    "\n",
    "* Старайтесь сделать код как можно более оптимальным. В частности, будет штрафоваться использование циклов в тех случаях, когда операцию можно совершить при помощи инструментов библиотек, о которых рассказывалось в курсе.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8Nfs9riBp0k"
   },
   "outputs": [],
   "source": [
    "from typing import Iterable, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5OOYVQOBp0l"
   },
   "source": [
    "## Часть 1. Градиентный спуск (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j2vGNTyBp0l"
   },
   "source": [
    "Для начала давайте вспомним самый простой функционал ошибки, который мы применяем в задаче регрессии — Mean Squared Error:\n",
    "\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell (\\langle x_i, w \\rangle - y_i)^2\n",
    "$$\n",
    "\n",
    "где $x_i$ — это $i$-ый объект датасета, $y_i$ — правильный ответ для $i$-го объекта, а $w$ — веса нашей линейной модели.\n",
    "\n",
    "Как мы помним, для линейной модели, его можно записать в матричном виде вот так:\n",
    "\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{\\ell} || Xw - y ||^2\n",
    "$$\n",
    "\n",
    "где $X$ — это матрица объекты-признаки, а $y$ — вектор правильных ответов\n",
    "\n",
    "Для того чтобы воспользоваться методом градиентного спуска, нам нужно посчитать градиент нашего функционала. Для MSE он будет выглядеть так:\n",
    "\n",
    "$$\n",
    "\\nabla_w Q(w, X, y) = \\frac{2}{\\ell} X^T(Xw-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05tP14CdBp0l"
   },
   "source": [
    "Ниже приведён базовый класс `BaseLoss`, который мы будем использовать для реализации всех наших лоссов. Менять его не нужно. У него есть два абстрактных метода:\n",
    "1. Метод `calc_loss`, который будет принимать на вход объекты `x`, правильные ответы `y` и веса `w` и вычислять значения лосса\n",
    "2. Метод `calc_grad`, который будет принимать на вход объекты `x`, правильные ответы `y` и веса `w` и вычислять значения градиента (вектор)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWmOOrj6Bp0m"
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "\n",
    "class BaseLoss(abc.ABC):\n",
    "    \"\"\"Базовый класс лосса\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Функция для вычислений значения лосса\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: число -- значения функции потерь\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вычислений градиента лосса по весам w\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ob65L_82Bp0m"
   },
   "source": [
    "Теперь давайте напишем реализацию этого абстрактоного класса: Mean Squared Error лосс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezASiSCgBp0m"
   },
   "source": [
    "**Задание 1.1 (5/8 балла):** Реализуйте класс `MSELoss`\n",
    "\n",
    "Он должен вычислять лосс и градиент по формулам наверху"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpvu5XryBp0m"
   },
   "outputs": [],
   "source": [
    "class MSELoss(BaseLoss):\n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Функция для вычислений значения лосса\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: число -- значения функции потерь\n",
    "        \"\"\"\n",
    "        # -- YOUR CODE HERE --\n",
    "        # Вычислите значение функции потерь при помощи X, y и w и верните его\n",
    "\n",
    "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вычислений градиента лосса по весам w\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
    "        \"\"\"\n",
    "        # -- YOUR CODE HERE --\n",
    "        # Вычислите значение вектора градиента при помощи X, y и w и верните его"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "188svk-jBp0n"
   },
   "source": [
    "Теперь мы можем создать объект `MSELoss` и при помощи него вычислять значение нашей функции потерь и градиенты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoQZnHVRBp0n"
   },
   "outputs": [],
   "source": [
    "# Создадим объект лосса\n",
    "loss = MSELoss()\n",
    "\n",
    "# Создадим какой-то датасет\n",
    "X = np.arange(200).reshape(20, 10)\n",
    "y = np.arange(20)\n",
    "\n",
    "# Создадим какой-то вектор весов\n",
    "w = np.arange(10)\n",
    "\n",
    "# Выведем значение лосса и градиента на этом датасете с этим вектором весов\n",
    "print(loss.calc_loss(X, y, w))\n",
    "print(loss.calc_grad(X, y, w))\n",
    "\n",
    "# Проверка, что методы реализованы правильно\n",
    "assert loss.calc_loss(X, y, w) == 27410283.5, \"Метод calc_loss реализован неверно\"\n",
    "assert np.allclose(\n",
    "    loss.calc_grad(X, y, w),\n",
    "    np.array(\n",
    "        [\n",
    "            1163180.0,\n",
    "            1172281.0,\n",
    "            1181382.0,\n",
    "            1190483.0,\n",
    "            1199584.0,\n",
    "            1208685.0,\n",
    "            1217786.0,\n",
    "            1226887.0,\n",
    "            1235988.0,\n",
    "            1245089.0,\n",
    "        ]\n",
    "    ),\n",
    "), \"Метод calc_grad реализован неверно\"\n",
    "print(\"Всё верно!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqRkUQpABp0n"
   },
   "source": [
    "Теперь когда у нас есть всё для вычисления градиента, давайте напишем наш градиентный спуск. Напомним, что формула для одной итерации градиентного спуска выглядит следующим образом:\n",
    "\n",
    "$$\n",
    "w^t = w^{t-1} - \\eta \\nabla_{w} Q(w^{t-1}, X, y)\n",
    "$$\n",
    "\n",
    "Где $w^t$ — значение вектора весов на $t$-ой итерации, а $\\eta$ — параметр learning rate, отвечающий за размер шага."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9gOWGzqBp0n"
   },
   "source": [
    "**Задание 1.2 (5/8 балла):** Реализуйте функцию `gradient_descent`\n",
    "\n",
    "Функция должна принимать на вход начальное значение весов линейной модели `w_init`, матрицу объектов-признаков `X`, \n",
    "вектор правильных ответов `y`, объект функции потерь `loss`, размер шага `lr` и количество итераций `n_iterations`.\n",
    "\n",
    "Функция должна реализовывать цикл, в котором происходит шаг градиентного спуска (градиенты берутся из `loss` посредством вызова метода `calc_grad`) по формуле выше и возвращать \n",
    "траекторию спуска (список из новых значений весов на каждом шаге)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wxu_8RHkBp0o"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    w_init: np.ndarray,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    loss: BaseLoss,\n",
    "    lr: float,\n",
    "    n_iterations: int = 100000,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Функция градиентного спуска\n",
    "    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\n",
    "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
    "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
    "    :param loss: Объект подкласса BaseLoss, который умеет считать градиенты при помощи loss.calc_grad(X, y, w)\n",
    "    :param lr: float -- параметр величины шага, на который нужно домножать градиент\n",
    "    :param n_iterations: int -- сколько итераций делать\n",
    "    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
    "    \"\"\"\n",
    "    # -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM-gLeXPBp0o"
   },
   "source": [
    "Теперь создадим синтетический датасет и функцию, которая будет рисовать траекторию градиентного спуска по истории:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgaCBMiDBp0o"
   },
   "outputs": [],
   "source": [
    "# Создаём датасет из двух переменных и реального вектора зависимости w_true\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "n_features = 2\n",
    "n_objects = 300\n",
    "batch_size = 10\n",
    "num_steps = 43\n",
    "\n",
    "w_true = np.random.normal(size=(n_features,))\n",
    "\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :]\n",
    "y = X.dot(w_true) + np.random.normal(0, 1, (n_objects))\n",
    "w_init = np.random.uniform(-2, 2, (n_features))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCsiY4PBBp0o"
   },
   "outputs": [],
   "source": [
    "loss = MSELoss()\n",
    "w_list = gradient_descent(w_init, X, y, loss, 0.01, 100)\n",
    "print(loss.calc_loss(X, y, w_list[0]))\n",
    "print(loss.calc_loss(X, y, w_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZm3EqMwBp0p"
   },
   "outputs": [],
   "source": [
    "def plot_gd(w_list: Iterable, X: np.ndarray, y: np.ndarray, loss: BaseLoss):\n",
    "    \"\"\"\n",
    "    Функция для отрисовки траектории градиентного спуска\n",
    "    :param w_list: Список из объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
    "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
    "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
    "    :param loss: Объект подкласса BaseLoss, который умеет считать лосс при помощи loss.calc_loss(X, y, w)\n",
    "    \"\"\"\n",
    "    w_list = np.array(w_list)\n",
    "    meshgrid_space = np.linspace(-2, 2, 100)\n",
    "    A, B = np.meshgrid(meshgrid_space, meshgrid_space)\n",
    "\n",
    "    levels = np.empty_like(A)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            w_tmp = np.array([A[i, j], B[i, j]])\n",
    "            levels[i, j] = loss.calc_loss(X, y, w_tmp)\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.title(\"GD trajectory\")\n",
    "    plt.xlabel(r\"$w_1$\")\n",
    "    plt.ylabel(r\"$w_2$\")\n",
    "    plt.xlim(w_list[:, 0].min() - 0.1, w_list[:, 0].max() + 0.1)\n",
    "    plt.ylim(w_list[:, 1].min() - 0.1, w_list[:, 1].max() + 0.1)\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "    # visualize the level set\n",
    "    CS = plt.contour(\n",
    "        A, B, levels, levels=np.logspace(0, 1, num=20), cmap=plt.cm.rainbow_r\n",
    "    )\n",
    "    CB = plt.colorbar(CS, shrink=0.8, extend=\"both\")\n",
    "\n",
    "    # visualize trajectory\n",
    "    plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "    plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqufHS3PBp0p"
   },
   "source": [
    "**Задание 1.3 (5/8 балла):** При помощи функций `gradient_descent` и  `plot_gd` нарисуйте траекторию градиентного спуска для разных значений длины шага (параметра `lr`). Используйте не менее четырёх разных значений для `lr`. \n",
    "\n",
    "Сделайте и опишите свои выводы о том, как параметр `lr` влияет на поведение градиентного спуска\n",
    "\n",
    "Подсказки:\n",
    "* Функция `gradient_descent` возвращает историю весов, которую нужно подать в функцию `plot_gd`\n",
    "* Хорошие значения для `lr` могут лежать в промежутке от 0.0001 до 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPtpweFLBp0q"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdxxpTlSBp0q"
   },
   "source": [
    "Теперь реализуем стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNXcI8AqBp0q"
   },
   "source": [
    "**Задание 1.4 (5/8 балла):** Реализуйте функцию `stochastic_gradient_descent`\n",
    "\n",
    "Функция должна принимать все те же параметры, что и функция `gradient_descent`, но ещё параметр `batch_size`, отвечающий за размер батча. \n",
    "\n",
    "Функция должна как и раньше реализовывать цикл, в котором происходит шаг градиентного спуска, но на каждом шаге считать градиент не по всей выборке `X`, а только по случайно выбранной части.\n",
    "\n",
    "Подсказка: для выбора случайной части можно использовать [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) с правильным параметром `size`, чтобы выбрать случайные индексы, а потом проиндексировать получившимся массивом массив `X`:\n",
    "```\n",
    "batch_indices = np.random.choice(X.shape[0], size=batch_size, replace=False)\n",
    "batch = X[batch_indices]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNlOVmKLBp0q"
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(\n",
    "    w_init: np.ndarray,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    loss: BaseLoss,\n",
    "    lr: float,\n",
    "    batch_size: int,\n",
    "    n_iterations: int = 1000,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Функция градиентного спуска\n",
    "    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\n",
    "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
    "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
    "    :param loss: Объект подкласса BaseLoss, который умеет считать градиенты при помощи loss.calc_grad(X, y, w)\n",
    "    :param lr: float -- параметр величины шага, на который нужно домножать градиент\n",
    "    :param batch_size: int -- размер подвыборки, которую нужно семплировать на каждом шаге\n",
    "    :param n_iterations: int -- сколько итераций делать\n",
    "    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
    "    \"\"\"\n",
    "    # -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhnQEuAFBp0r"
   },
   "source": [
    "**Задание 1.5 (5/8 балла):** При помощи функций `stochastic_gradient_descent` и  `plot_gd` нарисуйте траекторию градиентного спуска для разных значений длины шага (параметра `lr`) и размера подвыборки (параметра `batch_size`). Используйте не менее четырёх разных значений для `lr` и `batch_size`. \n",
    "\n",
    "Сделайте и опишите свои выводы о том, как параметры  `lr` и `batch_size` влияют на поведение стохастического градиентного спуска. Как отличается поведение стохастического градиентного спуска от обычного?\n",
    "\n",
    "Обратите внимание, что в нашем датасете всего 300 объектов, так что `batch_size` больше этого числа не будет иметь смысла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysPrEpkjBp0r"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZWZE2VDBp0r"
   },
   "source": [
    "Вы могли заметить, что поведение градиентного спуска, особенно стохастической версии, очень сильно зависит от размера шага. \n",
    "\n",
    "Как правило, в начале спуска мы хотим делать большие шаги, чтобы поскорее подойти поближе к минимуму, а позже мы уже хотим делать шаги маленькие, чтобы более точнее этого минимума достичь и не \"перепрыгнуть\" его. \n",
    "\n",
    "Чтобы достичь такого поведения мы можем постепенно уменьшать длину шага с увеличением номера итерации. Сделать это можно, например, вычисляя на каждой итерации длину шага по следующей формуле:\n",
    "\n",
    "$$\n",
    "    \\eta_t\n",
    "    =\n",
    "    \\lambda\n",
    "    \\left(\n",
    "        \\frac{s_0}{s_0 + t}\n",
    "    \\right)^p\n",
    "$$\n",
    "\n",
    "где $\\eta_t$ — длина шага на итерации $t$, $\\lambda$ — начальная длина шага (параметр `lr` у нас), $s_0$ и $p$ — настраиваемые параметры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Hqj3zJ4Bp0r"
   },
   "source": [
    "**Задание 1.6 (5/8 балла):** Реализуйте функцию `stochastic_gradient_descent` на этот раз с затухающим шагом по формуле выше. Параметр $s_0$ возьмите равным 1. Параметр $p$ возьмите из нового аргумента функции `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uL65tOmBp0r"
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(\n",
    "    w_init: np.ndarray,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    loss: BaseLoss,\n",
    "    lr: float,\n",
    "    batch_size: int,\n",
    "    p: float,\n",
    "    n_iterations: int = 1000,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Функция градиентного спуска\n",
    "    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\n",
    "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
    "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
    "    :param loss: Объект подкласса BaseLoss, который умеет считать градиенты при помощи loss.calc_grad(X, y, w)\n",
    "    :param lr: float -- параметр величины шага, на который нужно домножать градиент\n",
    "    :param batch_size: int -- размер подвыборки, которую нужно семплировать на каждом шаге\n",
    "    :param p: float -- значение степени в формуле затухания длины шага\n",
    "    :param n_iterations: int -- сколько итераций делать\n",
    "    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
    "    \"\"\"\n",
    "    # -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwgaqqB5Bp0r"
   },
   "source": [
    "**Задание 1.7 (5/8 балла):** При помощи новой функции `stochastic_gradient_descent` и функции `plot_gd` нарисуйте траекторию градиентного спуска для разных значений параметра `p`. Используйте не менее четырёх разных значений для `p`. Хорошими могут быть значения, лежащие в промежутке от 0.1 до 1.\n",
    "Параметр `lr` возьмите равным 0.01, а параметр `batch_size` равным 10.\n",
    "\n",
    "Сделайте и опишите свои выводы о том, как параметр `p` влияет на поведение стохастического градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ri2bF6iCBp0s"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0YKLC4mBp0s"
   },
   "source": [
    "**Задание 1.8 (5/8 балла):** Сравните сходимость обычного градиентного спуска и стохастичекой версии:\n",
    "Нарисуйте график зависимости значения лосса (его можно посчитать при помощи метода `calc_loss`, используя $x$ и $y$ из датасета и $w$ с соответствующей итерации) от номера итерации для траекторий, полученных при помощи обычного и стохастического градиентного спуска с одинаковыми параметрами. Параметр `batch_size` возьмите равным 10.\n",
    "\n",
    "Видно ли на данном графике преимущество SGD? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_f0qCQsBp0s"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7jS7bJUBp0s"
   },
   "source": [
    "## Часть 2. Линейная регрессия (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRrNZdJ7Bp0s"
   },
   "source": [
    "Теперь давайте напишем наш класс для линейной регрессии. Он будет использовать интерфейс, знакомый нам из библиотеки `sklearn`.\n",
    "\n",
    "В методе `fit` мы будем подбирать веса `w` при помощи градиентного спуска нашим методом `gradient_descent`\n",
    "\n",
    "В методе `predict` мы будем применять нашу регрессию к датасету, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyKvDkPOBp0s"
   },
   "source": [
    "**Задание 2.1 (5/8 балла):** Допишите код в методах `fit` и `predict` класса `LinearRegression`\n",
    "\n",
    "В методе `fit` вам нужно как-то инициализировать веса `w`, применить `gradient_descent` и сохранить последнюю `w` из траектории.\n",
    "\n",
    "В методе `predict` вам нужно применить линейную регрессию и вернуть вектор ответов.\n",
    "\n",
    "Обратите внимание, что объект лосса передаётся в момент инициализации и хранится в `self.loss`. Его нужно использовать в `fit` для `gradient_descent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KdTk79QTBp0t"
   },
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, loss: BaseLoss, lr: float = 0.1) -> None:\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"LinearRegression\":\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        # Добавляем столбец из единиц для константного признака\n",
    "        X = np.hstack([X, np.ones([X.shape[0], 1])])\n",
    "\n",
    "        # -- YOUR CODE HERE --\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        # Проверяем, что регрессия обучена, то есть, что был вызван fit и в нём был установлен атрибут self.w\n",
    "        assert hasattr(self, \"w\"), \"Linear regression must be fitted first\"\n",
    "        # Добавляем столбец из единиц для константного признака\n",
    "        X = np.hstack([X, np.ones([X.shape[0], 1])])\n",
    "\n",
    "        # -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV2RGHwgBp0t"
   },
   "source": [
    "Теперь у нас есть наш класс линейной регрессии. Более того, мы можем управлять тем, какую функцию потерь мы оптимизируем, просто передавая разные классы в параметр `loss` при инициализации. \n",
    "\n",
    "Пока у нас нет никаких классов кроме `MSELoss`, но скоро они появятся.\n",
    "\n",
    "Для `MSELoss` мы бы создавали наш объект линейной регрессии, например, так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cyfkw-L1Bp0t"
   },
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression(MSELoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIG_P49YBp0t"
   },
   "source": [
    "Применим нашу регрессию на реальном датасете. Загрузим датасет с машинами, который был у вас на семинарах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJn0SUlnBp0t"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_raw = pd.read_csv(\n",
    "    \"http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\",\n",
    "    header=None,\n",
    "    na_values=[\"?\"],\n",
    ")\n",
    "X_raw.head()\n",
    "X_raw = X_raw[~X_raw[25].isna()].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KcER5JtBp0u"
   },
   "outputs": [],
   "source": [
    "y = X_raw[25]\n",
    "X_raw = X_raw.drop(25, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVS4T-IUBp0u"
   },
   "source": [
    "**Задание 2.2 (5/8 балла):** Как обычно обработайте датасет всеми нужными методами, чтобы на нём можно было обучать линейную регрессию:\n",
    "\n",
    "* Разделите датасет на обучающую и тестовую выборку\n",
    "* Заполните пропуски\n",
    "* Нормализуйте числовые признаки\n",
    "* Закодируйте категориальные переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COp1ybClBp0u"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qm7c7zIzBp0u"
   },
   "source": [
    "**Задание 2.3 (5/8 балла):** Обучите написанную вами линейную регрессию на обучающей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lzjAlpliBp0u"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rr1bF2_bBp0u"
   },
   "source": [
    "**Задание 2.4 (5/8 балла):** Посчитайте ошибку обученной регрессии на обучающей и тестовой выборке при помощи метода `mean_squared_error` из `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2KjQjxvBp0v"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SS5Oh4YoBp0v"
   },
   "source": [
    "Наша модель переобучилась. Давайте как обычно в такой ситуации добавим к ней L2 регуляризацию. Для этого нам нужно написать новый класс лосса.\n",
    "\n",
    "Формула функции потерь для MSE с L2 регуляризацией выглядит так:\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell (\\langle x_i, w \\rangle - y_i)^2 + \\lambda ||w||^2\n",
    "$$\n",
    "\n",
    "Или в матричном виде:\n",
    "\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{\\ell} || Xw - y ||^2 + \\lambda ||w||^2\n",
    "$$\n",
    "\n",
    "Где $\\lambda$ — коэффициент регуляризации\n",
    "\n",
    "Градиент выглядит так:\n",
    "\n",
    "$$\n",
    "\\nabla_w Q(w, X, y) = \\frac{2}{\\ell} X^T(Xw-y) + 2 \\lambda w\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vogzj2W0Bp0v"
   },
   "source": [
    "**Задание 2.5 (5/8 балла):** Реализуйте класс `MSEL2Loss`\n",
    "\n",
    "Он должен вычислять лосс и градиент по формулам наверху\n",
    "\n",
    "Подсказка: обратите внимание, что последний элемент вектора `w` — это bias (в классе `LinearRegression` к матрице `X` добавляется колонка из единиц — константный признак). Как мы знаем из лекций и семинаров, bias регуляризовать не нужно. Поэтому не забудьте убрать последний элемент из `w` при подсчёте слагаемого $\\lambda||w||^2$ в `calc_loss` и занулить его при подсчёте слагаемого $2 \\lambda w$ в `calc_grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58FvnEjHBp0v"
   },
   "outputs": [],
   "source": [
    "class MSEL2Loss(BaseLoss):\n",
    "    def __init__(self, coef: float = 1.0):\n",
    "        \"\"\"\n",
    "        :param coef: коэффициент регуляризации (лямбда в формуле)\n",
    "        \"\"\"\n",
    "        self.coef = coef\n",
    "\n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Функция для вычислений значения лосса\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета. Последний признак константный.\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии. Последний вес -- bias.\n",
    "        :output: число -- значения функции потерь\n",
    "        \"\"\"\n",
    "        # -- YOUR CODE HERE --\n",
    "        # Вычислите значение функции потерь при помощи X, y и w и верните его\n",
    "\n",
    "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вычислений градиента лосса по весам w\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :output: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
    "        \"\"\"\n",
    "        # -- YOUR CODE HERE --\n",
    "        # Вычислите значение вектора градиента при помощи X, y и w и верните его"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTl1XQP5Bp0v"
   },
   "source": [
    "Теперь мы можем использовать лосс с l2 регуляризацией в нашей регрессии, например, так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QanspiddBp0v"
   },
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression(MSEL2Loss(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwmC8B0OBp0w"
   },
   "source": [
    "**Задание 2.6 (5/8 балла):** Обучите регрессию с лоссом `MSEL2Loss`. Подберите хороший коэффициент регуляризации и добейтесь улучшения результата на тестовой выборке. Сравните результат на обучающей и тестовой выборке с регрессией без регуляризации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FId8tKOBp0w"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3GSc1JvBp0w"
   },
   "source": [
    "В нашем датасете могут быть выбросы. На семинаре вам рассказывали, что с выбросами хорошо помогает бороться Huber Loss. Вдали от нуля он работает как Mean Absolute Error и не реагирует на выбросы так сильно, как MSE. Давайте его реализуем и применим в нашей регрессии.\n",
    "\n",
    "Напомним, что функция потерь Huber Loss'а  выглядит так:\n",
    "\n",
    "\n",
    "$$\n",
    "    Q(w, X, y) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell \\phi_\\varepsilon (\\langle x_i, w \\rangle - y_i)\n",
    "$$\n",
    "$$\n",
    "    \\phi_\\varepsilon(z) = \\begin{cases} \\frac 1 2 z^2, - \\varepsilon < z < \\varepsilon, \\\\\\varepsilon (|z| - \\frac 1 2 \\varepsilon), иначе \\\\ \\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "А градиент так:\n",
    "$$\n",
    "    \\nabla_w Q(w, X, y) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell x_i \\nabla_z \\phi_\\varepsilon (\\langle x_i, w \\rangle - y_i)\n",
    "$$\n",
    "$$\n",
    "    \\nabla_z \\phi_\\varepsilon(z) = \\begin{cases} z, - \\varepsilon < z < \\varepsilon, \\\\\\varepsilon \\text{ sign}(z), иначе \\\\ \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-y7M1qIBp0w"
   },
   "source": [
    "**Задание 2.7 (5/8 балла):** Реализуйте класс `HuberLoss`\n",
    "\n",
    "Он должен вычислять лосс и градиент по формулам наверху"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqXGo9OLBp0w"
   },
   "outputs": [],
   "source": [
    "class HuberLoss(BaseLoss):\n",
    "    def __init__(self, eps: float) -> None:\n",
    "        \"\"\"\n",
    "        :param eps: параметр huber loss из формулы\n",
    "        \"\"\"\n",
    "        self.eps = eps\n",
    "\n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Функция для вычислений значения лосса\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :output: число -- значения функции потерь\n",
    "        \"\"\"\n",
    "        # -- YOUR CODE HERE --\n",
    "        # Вычислите значение функции потерь при помощи X, y и w и верните его\n",
    "\n",
    "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вычислений градиента лосса по весам w\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :output: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
    "        \"\"\"\n",
    "        # -- YOUR CODE HERE --\n",
    "        # Вычислите значение вектора градиента при помощи X, y и w и верните его"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyZkVwLWBp0w"
   },
   "source": [
    "**Задание 2.8 (5/8 балла):** Обучите регрессию с лоссом `HuberLoss`. Сравните результат на обучающей и тестовой выборке с регрессией, обученной c `MSELoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_BLLnGuBp0w"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdIqCkSEBp0w"
   },
   "source": [
    "**Задание 3 (0.08/8 балла)**\n",
    "Вставьте ваш любимый мем 2021 в ячейку ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "numfffXPBp0x"
   },
   "outputs": [],
   "source": [
    "# -- YOUR MEME HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5cGx9vyBp0x"
   },
   "source": [
    "### БОНУС (2.5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzpUzkPABp0x"
   },
   "source": [
    "Градиентный спуск — далеко не единственный метод оптимизации. \n",
    "Другой очень известный метод называется [\"Алгоритм имитации отжига\"](https://ru.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC_%D0%B8%D0%BC%D0%B8%D1%82%D0%B0%D1%86%D0%B8%D0%B8_%D0%BE%D1%82%D0%B6%D0%B8%D0%B3%D0%B0). Он не так часто используется для оптимизации моделей машинного обучения, но у вас есть уникальная возможность попробовать применить его к нашей любимой линейной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7Mr6bQLBp0x"
   },
   "source": [
    "**Задание (2.5 баллов)**:\n",
    "Напишите алгоритм имитации отжига для оптимизации MSE линейной регрессии. \n",
    "\n",
    "Сравните результат с градиентным спуском по \"траектории\" и по финальному лоссу.\n",
    "\n",
    "Подсказка: каждую новую точку (веса регресси в нашем случае) можно семплировать из некоторого случайного распределением с центром в текущей точке. Хорошо подойдут распределения с \"тяжёлыми\" хвостами, например, распределение Стьюдента с параметром количества степеней свободы в районе 3.\n",
    "Это может выглядеть, например, так:\n",
    "```\n",
    "new_w = old_w + np.random.standard_t(3, size=old_w.shape)\n",
    "```\n",
    "С параметром распределения можно поэксперементировать: чем он больше, тем реже новые точки будут очень сильно уходить от старых."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xuSuYPzBp0x"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "l5cGx9vyBp0x"
   ],
   "name": "hw05-gd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
