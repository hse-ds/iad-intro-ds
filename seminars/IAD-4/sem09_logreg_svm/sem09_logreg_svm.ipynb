{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар: логистическая регрессия и SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная классификация\n",
    "\n",
    "### Постановка задачи классификации\n",
    "\n",
    "Пусть задана обучающая выборка $X = \\left\\{ \\left( x_i, y_i \\right) \\right\\}_{i=1}^l, x_i \\in \\mathbb{X}, y_i \\in \\mathbb{Y},$ — $l$ пар объект-ответ, где\n",
    "$\\mathbb{X}$ — пространство объектов,\n",
    "$\\mathbb{Y}$ — пространство ответов.\n",
    "\n",
    "\n",
    "### Логистическая регрессия\n",
    "\n",
    "Рассмотрим в качестве верхней оценки пороговой функции потерь логистическую функцию:\n",
    "\n",
    "$$\\tilde{L}(M) = \\log (1 + \\exp(-M)).$$\n",
    "\n",
    "Таким образом, необходимо решить следующую оптимизационную задачу:\n",
    "$$\\frac{1}{l} \\sum_{i=1}^l \\tilde{L} (M_i) = \\frac{1}{l} \\sum_{i=1}^l \\log (1 + \\exp (-y_i \\langle w, x_i \\rangle)) \\to \\min_w$$\n",
    "\n",
    "Получившийся метод обучения называется **логистической регрессией**.\n",
    "\n",
    "Одно из полезных свойств логистической регрессии, которое будет изучено нами несколько позднее, — тот факт, что она позволяет предсказывать помимо метки класса ещё и вероятность принадлежности каждому из них, что может быть полезным в некоторых задачах.\n",
    "\n",
    "**Пример**: Вы работаете в банке и хотите выдавать кредиты только тем клиентам, которые вернут его с вероятностью не меньше 0.9.\n",
    "\n",
    "Попробуем сконструировать функцию потерь из других соображений.\n",
    "Если алгоритм $b(x) \\in [0, 1]$ действительно выдает вероятности, то\n",
    "они должны согласовываться с выборкой.\n",
    "С точки зрения алгоритма вероятность того, что в выборке встретится объект $x_i$ с классом $y_i$,\n",
    "равна $b(x_i)^{[y_i = +1]} (1 - b(x_i))^{[y_i = -1]}$.\n",
    "\n",
    "Исходя из этого, можно записать правдоподобие выборки (т.е. вероятность получить такую выборку\n",
    "с точки зрения алгоритма):\n",
    "$$\n",
    "    Q(a, X)\n",
    "    =\n",
    "    \\prod_{i = 1}^{\\ell}\n",
    "        b(x_i)^{[y_i = +1]} (1 - b(x_i))^{[y_i = -1]}.\n",
    "$$\n",
    "Данное правдоподобие можно использовать как функционал для обучения алгоритма --\n",
    "с той лишь оговоркой, что удобнее оптимизировать его логарифм:\n",
    "$$\n",
    "    -\\sum_{i = 1}^{\\ell} \\left(\n",
    "        [y_i = +1] \\log b(x_i)\n",
    "        +\n",
    "        [y_i = -1] \\log (1 - b(x_i))\n",
    "    \\right)\n",
    "    \\to\n",
    "    \\min\n",
    "$$\n",
    "Данная функция потерь называется логарифмической (log-loss).\n",
    "\n",
    "Мы хотим предсказывать вероятности, то есть, чтобы наш алгоритм предсказывал числа в интервале [0, 1]. Этого легко достичь, если положить $b(x) = \\sigma(\\langle w, x \\rangle)$,\n",
    "где в качестве $\\sigma$ может выступать любая монотонно неубывающая функция\n",
    "с областью значений $[0, 1]$.\n",
    "Мы будем использовать сигмоидную функцию: $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$.\n",
    "Таким образом, чем больше скалярное произведение $\\langle w, x \\rangle$,\n",
    "тем больше будет предсказанная вероятность.\n",
    "\n",
    "Подставим трансформированный ответ линейной модели в логарифмическую функцию потерь:\n",
    "\\begin{align*}\n",
    "    -\\sum_{i = 1}^{\\ell} &\\left(\n",
    "        [y_i = +1]\n",
    "        \\log \\frac{1}{1 + \\exp(-\\langle w, x_i \\rangle)}\n",
    "        +\n",
    "        [y_i = -1]\n",
    "        \\log \\frac{\\exp(-\\langle w, x_i \\rangle)}{1 + \\exp(-\\langle w, x_i \\rangle)}\n",
    "    \\right)\n",
    "    =\\\\\n",
    "    &=\n",
    "    -\\sum_{i = 1}^{\\ell} \\left(\n",
    "        [y_i = +1]\n",
    "        \\log \\frac{1}{1 + \\exp(-\\langle w, x_i \\rangle)}\n",
    "        +\n",
    "        [y_i = -1]\n",
    "        \\log \\frac{1}{1 + \\exp(\\langle w, x_i \\rangle)}\n",
    "    \\right)\n",
    "    =\\\\\n",
    "    &=\n",
    "    \\sum_{i = 1}^{\\ell} \\left(\n",
    "        [y_i = +1]\n",
    "        \\log (1 + \\exp(-\\langle w, x_i \\rangle))\n",
    "        +\n",
    "        [y_i = -1]\n",
    "        \\log (1 + \\exp(\\langle w, x_i \\rangle))\n",
    "    \\right)\n",
    "    =\\\\\n",
    "    &=\n",
    "    \\sum_{i = 1}^{\\ell}\n",
    "        \\log \\left(\n",
    "            1 + \\exp(-y_i \\langle w, x_i \\rangle)\n",
    "        \\right).\n",
    "\\end{align*}\n",
    "\n",
    "Полученная функция в точности представляет собой логистические потери,\n",
    "упомянутые в начале.\n",
    "Линейная модель классификации, настроенная путём минимизации данного функционала,\n",
    "называется логистической регрессией.\n",
    "Как видно из приведенных рассуждений, она оптимизирует\n",
    "правдоподобие выборки и дает корректные оценки вероятности принадлежности к положительному классу.\n",
    "\n",
    "\n",
    "### Пример обучения логистической регрессии\n",
    "#### Определение спама по тексту электронного письма\n",
    "\n",
    "Попробуем при помощи моделей линейной классификации построить алгоритм, отделяющий спам от нормальной почты. Для экспериментов воспользуемся небольшим набором данных с [UCI](https://archive.ics.uci.edu/ml/datasets.html). Объекты в датасете соответствуют письмам, которые описаны признаками на основе текста письма, спам — положительный пример для классификации, хорошее письмо — отрицательный пример.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from urllib.request import urlopen\n",
    "\n",
    "SPAMBASE_NAMES_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names'\n",
    "SPAMBASE_DATA_URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
    "\n",
    "feature_names = [\n",
    "    line.strip().split(b':')[0].decode(\"utf-8\")\n",
    "    for line in urlopen(SPAMBASE_NAMES_URL).readlines()[33:]\n",
    "]\n",
    "spam_data = pandas.read_csv(SPAMBASE_DATA_URL, header=None, names=(feature_names + ['spam']))\n",
    " \n",
    "# X, y = spam_data.ix[:, :-1].values, spam_data.ix[:, -1].values\n",
    "X, y = spam_data.iloc[:, :-1].values, spam_data.iloc[:, -1].values\n",
    " \n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение логистической регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим выборку на обучающую и тестовую в отношении 80/20 и обучим логистическую регрессию при помощи объекта [LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# splitting data\n",
    "train_part = 0.8\n",
    "n_train = int(train_part * X.shape[0])\n",
    "X_tr = X[:n_train]\n",
    "X_test = X[n_train:]\n",
    "y_tr = y[:n_train]\n",
    "y_test = y[n_train:]\n",
    "\n",
    "# training\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.shape, y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим долю верных прогнозов полученной модели (accuracy) при помощи соответствующей функции из модуля [sklearn.metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "print (f'Accuracy = {metrics.accuracy_score(y_test, preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем двумерную искуственную выборку из 2 различных нормальных распределений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mean0 = [10, 5]\n",
    "cov0 = [[1, 0], [0, 5]]  # diagonal covariance\n",
    "data0 = np.random.multivariate_normal(mean0, cov0, 1000)\n",
    "mean1 = [0, 0]\n",
    "cov1 = [[3, 1], [0, 1]]\n",
    "data1 = np.random.multivariate_normal(mean1, cov1, 1000)\n",
    "data = np.vstack((data0, data1))\n",
    "y_vis = np.hstack((-np.ones(1000), np.ones(1000)))\n",
    "\n",
    "\n",
    "plt.scatter(data0[:, 0], data0[:, 1], c='red')\n",
    "plt.scatter(data1[:, 0], data1[:, 1], c='green')\n",
    "plt.legend(['y = -1', 'y = 1'])\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-5,15])\n",
    "axes.set_ylim([-5,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим логистическую регрессию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vis, X_test_vis, y_train_vis, y_test_vis = train_test_split(data, y_vis, test_size=0.2)\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "logreg = linear_model.LogisticRegression(penalty = 'l2')\n",
    "logreg.fit(X_train_vis, y_train_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученные в результате оптимизации коэффициенты линейной модели содержатся в атрибутах coef\\_ и intercept\\_ соответствующего объекта. Визуализируем разделяющую гиперплоскость алгоритма и рассмотрим значения предсказанных моделью вероятностей принадлежности нового объекта каждому из классов в зависимости от его координат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logreg.coef_, logreg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = logreg.coef_[0][0]\n",
    "w_2 = logreg.coef_[0][1]\n",
    "w_0 = logreg.intercept_[0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(data0[:, 0], data0[:, 1], c='red')\n",
    "plt.scatter(data1[:, 0], data1[:, 1], c='green')\n",
    "plt.legend(['y = -1', 'y = 1'])\n",
    "x_arr = np.linspace(-10, 15, 3000)\n",
    "plt.plot(x_arr, -(w_0 + w_1 * x_arr) / w_2)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-5,15])\n",
    "axes.set_ylim([-5,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = np.array([[10, 2]]) # изменяем только координаты объекта\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(data0[:, 0], data0[:, 1], c='red')\n",
    "plt.scatter(data1[:, 0], data1[:, 1], c='green')\n",
    "plt.scatter(point[:, 0], point[:, 1], marker = '*', s = 300, color = 'magenta')\n",
    "plt.legend(['y = -1', 'y = 1'])\n",
    "x_arr = np.linspace(-10, 15, 3000)\n",
    "plt.plot(x_arr, -(w_0 + w_1 * x_arr) / w_2)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-5,15])\n",
    "axes.set_ylim([-5,10])\n",
    "prob = logreg.predict_proba(point)\n",
    "print (f'P(y = -1|x) = {prob[0][0]}')\n",
    "print (f'P(y = 1|x) = {prob[0][1]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier\n",
    "\n",
    "Объект [SGDClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) позволяет обучать линейные модели классификации и регрессии с помощью стохастического градиентного спуска.\n",
    "\n",
    "Полезные параметры:\n",
    "* loss - функция потерь (по факту то, какую модель обучаем): **hinge** (SVM), **log** (логистическая регрессия), **perceptron** (персептрон) и другие;\n",
    "* penalty - тип регуляризации: **l1**, **l2**, **elasticnet** (смесь l1 и l2 регуляризации);\n",
    "* alpha - коэффициент регуляризации;\n",
    "* fit_intercept - необходимо ли добавлять в модель свободный член (True/False);\n",
    "* max_iter - число эпох (полных проходов по выборке) при обучении;\n",
    "* learning_rate - шаг градиентного спуска (оптимизируется по умолчанию)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "lr_sgd = SGDClassifier(loss=\"log\", alpha=0.05, max_iter=200, fit_intercept=True)\n",
    "lr_sgd.fit(X_tr, y_tr)\n",
    "preds_sgd = lr_sgd.predict(X_test)\n",
    "print('Accuracy =', metrics.accuracy_score(y_test, preds_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка качества работы классификатора\n",
    "### Бинарные метрики\n",
    "Напомним метрики, изученные нами на прошлом семинаре\n",
    "\n",
    "Часто рассматриваются следующие метрики качества бинарной классификации:\n",
    "  - Доля правильных ответов (Accuracy):\n",
    "  $$accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "  - Точность/precision (доля действительно положительных объектов среди объектов, выделенных алгоритмом):\n",
    "  $$precision = \\frac{TP}{TP + FP}$$\n",
    "  - Полнота/recall (доля выделенных алгоритмом объектов среди всех положительных объектов выборки):\n",
    "  $$recall = \\frac{TP}{TP + FN}$$\n",
    "  - $F_1$-мера (среднее гармоническое между точностью и полнотой)\n",
    "  $$F_1 = \\frac{2 \\cdot Precision \\cdot Recall}{Precision + Recall}$$\n",
    "\n",
    "\n",
    "\n",
    "Также распространены ROC-кривые и Precision/Recall кривые.\n",
    " - По осям Ox и Oy ROC-кривой отложены соответственно False Positive Rate (FPR) и True Positive Rate (TPR):\n",
    " $$FPR = \\frac{FP}{FP + TN},$$\n",
    " $$TPR = \\frac{TP}{FN + TP}.$$\n",
    " \n",
    " - По осям Ox и Oy PR-кривой отложены соответственно Recall и Precision.\n",
    "\n",
    " ![](https://glassboxmedicine.files.wordpress.com/2019/02/roc-curve-v2.png)\n",
    "\n",
    "В случае, если необходимо сравнить качество классификаторов вне зависимости от порога, применяют интегральные числовые метрики, например AUC-ROC (**A**rea **U**nder RO**C**) — площадь под ROC-кривой классификатора. AUC-ROC идеально работающего классификатора равно 1. Идеальный случайный классификатор в среднем имеет AUC-ROC=0.5.\n",
    "\n",
    "Построим описанные кривые для логистической регрессии, обученной на описанном выше датасете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PR-кривая проходит через точку (0,0) либо (0,1), а также точку (1,r), где $r$ - доля положительных объектов.\n",
    "\n",
    "* Если при каком-то значении порога $t$ алгоритм $a(x)$ идеально разделяет объекты 2 классов, то PR-кривая проходит через точку (1,1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train_curve, X_test_curve, y_train_curve, y_test_curve = train_test_split(X, y, test_size=0.2, random_state=20)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X_train_curve, y_train_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test_curve, lr.predict_proba(X_test_curve)[:, 1])\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('recall')\n",
    "plt.ylabel('precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "auc(recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ROC проходит через точки (0,0) и (1,1)\n",
    "* Если при каком-то значении порога $t$ алгоритм $a(x)$ идеально разделяет объекты 2 классов, то ROC проходит через точку (0,1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test_curve, lr.predict_proba(X_test_curve)[:, 1])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test_curve, lr.predict(X_test_curve)[:])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Валидация\n",
    "Чтобы оценить качество работы алгоритма, необходимо провести валидацию. Это один из самых важных шагов в процессе решения задачи. Оценим accuracy для модели логистической регрессии в задаче про спам-письма на тестовой выборке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data = pandas.read_csv(SPAMBASE_DATA_URL, header=None, names=(feature_names + ['spam']))\n",
    " \n",
    "X, y = spam_data.iloc[:, :-1].values, spam_data.iloc[:, -1].values\n",
    "\n",
    "# обучающая выборка\n",
    "X_tr = X[:n_train]\n",
    "y_tr = y[:n_train]\n",
    "\n",
    "# валидационная выборка\n",
    "X_test = X[n_train:]\n",
    "y_test = y[n_train:]\n",
    "\n",
    "# обучим ещё раз логистическую регрессию\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X_tr, y_tr)\n",
    "\n",
    "#посмотрим на точность классификации\n",
    "\n",
    "preds = lr.predict(X_test)\n",
    "print (f'Accuracy = {metrics.accuracy_score(y_test, preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь попробуем перемешать объекты и повторим действия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size=train_part, random_state=123)\n",
    "\n",
    "# обучим ещё раз логистическую регрессию\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X_tr, y_tr)\n",
    "\n",
    "#посмотрим на точность классификации\n",
    "\n",
    "preds = lr.predict(X_test)\n",
    "print (f'Accuracy = {metrics.accuracy_score(y_test, preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы видим, качество классификации новых данных резко возросло. С чем это может быть связано? Рассмотрим вектор целевой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y, 'ro')\n",
    "plt.xlabel('Object number')\n",
    "plt.ylabel('Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема заключалась в том, что в выборке примеры были упорядочены: сначала шли примеры положительного класса, а потом отрицательного. Поэтому нельзя забывать **перемешивать классы**.\n",
    "\n",
    "Чтобы повысить устойчивость оценки качества, можно проводить разбиение выборки на обучающую и тестовую не один, $N$ раз, после чего усреднять результаты, полученные на $N$ контрольных выборках. Для этого можно использовать функцию [`sklearn.model_selection.ShuffleSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = ShuffleSplit(n_splits=5, test_size=.1, random_state=123)\n",
    "rs.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quals = []\n",
    "lr = LogisticRegression()\n",
    "\n",
    "for tr_ind, test_ind in rs.split(X):\n",
    "    # using new set split for training and evaluating\n",
    "    lr.fit(X[tr_ind, :], y[tr_ind])\n",
    "    quals.append(metrics.roc_auc_score(y[test_ind], lr.predict_proba(X[test_ind,:])[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'Mean AUC-ROC = {np.mean(quals)}')\n",
    "print (f'AUC-ROC standart deviation = {np.std(quals)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим зависимость оценки качества от пропорции, в которой выборка разбивается на обучающую и тестовую:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "stds = []\n",
    "xs = []\n",
    "quals = []\n",
    "for tr_size in np.arange(0.5,0.9,0.01):\n",
    "    # set train size\n",
    "    rs = ShuffleSplit(n_splits=5, train_size=tr_size, random_state=123)\n",
    "    rs.get_n_splits(X)\n",
    "    fold_quals = []\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    for tr_ind, test_ind in rs.split(X):\n",
    "        # evaluate quality for each split\n",
    "        lr.fit(X[tr_ind, :], y[tr_ind])\n",
    "        qual = metrics.roc_auc_score(y[test_ind], lr.predict_proba(X[test_ind,:])[:,1])\n",
    "        fold_quals.append(qual)\n",
    "        quals.append(qual)\n",
    "        xs.append(tr_size)\n",
    "    # evaluation for current train set size\n",
    "    means.append(np.mean(fold_quals))\n",
    "    stds.append(np.std(fold_quals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0.5,0.9,0.01), means, 'ro')\n",
    "plt.xlabel('train size')\n",
    "plt.ylabel('AUC-ROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0.5,0.9,0.01), stds)\n",
    "plt.xlabel('train size')\n",
    "plt.ylabel('AUC-ROC std');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Несбалансированные классы\n",
    "\n",
    "Если объём выборки невелик, а объектов одного класса значительно меньше, чем другого, то может сложиться ситуация, когда при случайном разбиении объектов меньшего класса не окажется в тестовой выборке, в связи с чем результаты оценки качества будут неустойчивы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пример**: задача кредитного скоринга. Поскольку случаи невозврата кредита довольно редки, количество объектов отрицательного класса будет значительно меньше, чем положительного."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('givemesomecredit')\n",
    "X = df.drop('SeriousDlqin2yrs', axis=1)\n",
    "X = X.fillna(X.mean()).values\n",
    "y = df['SeriousDlqin2yrs']\n",
    "print (f'Доля заемщиков, не вернувших кредит: {y.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Всего 0.7% выборки составляют объекты положительного класса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- В таком случае необходимо производить стратификацию, то есть разбивать отдельно объекты каждого класса на обучение и тест (сохраняя их доли). Например, с помощью [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) или [`StratifiedShuffleSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим влияние стратификации на оценку качества путем разбиения выборки $N=10$ раз на обучение и тест и последующего усреднения AUC-ROC на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=10, train_size=0.8, random_state=124)\n",
    "quals = []\n",
    "\n",
    "lr = LogisticRegression()\n",
    "for tr_ind, test_ind in cv.split(X):\n",
    "    lr.fit(X[tr_ind, :], y[tr_ind])\n",
    "    quals.append(metrics.roc_auc_score(y[test_ind], lr.predict_proba(X[test_ind,:])[:,1]))\n",
    "\n",
    "print(\"AUC-ROC w/o stratification = \", np.mean(quals))\n",
    "print(\"AUC-ROC std w/o stratification = \", np.std(quals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=10, train_size=0.8, random_state=124)\n",
    "quals = []\n",
    "lr = LogisticRegression()\n",
    "for tr_ind, test_ind in cv.split(X, y):\n",
    "    print(\"Доля невозврата: \", y[tr_ind].mean())\n",
    "    lr.fit(X[tr_ind, :], y[tr_ind])\n",
    "    quals.append(metrics.roc_auc_score(y[test_ind], lr.predict_proba(X[test_ind,:])[:,1]))\n",
    "\n",
    "print(\"AUC-ROC with stratification = \", np.mean(quals))\n",
    "print(\"AUC-ROC std with stratification = \", np.std(quals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Настройка параметров модели\n",
    "\n",
    "Настройка наилучших параметров модели производится с использованием техник кросс-валидации (для оценки качества) и соответственно перебора параметров. \n",
    "\n",
    "Для перебора параметров можно использовать: \n",
    "  - Класс [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) — полный перебор параметров модели по сетке\n",
    "  - Класс [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) — перебирает случайные комбинации параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "Рассмотрим теперь другой подход к построению функции потерь,\n",
    "основанный на максимизации зазора между классами.\n",
    "Будем рассматривать линейные классификаторы вида\n",
    "$$\n",
    "    a(x) = sign (\\langle w, x \\rangle + b), \\qquad w \\in R^d, b \\in R.\n",
    "$$\n",
    "\n",
    "### Разделимый случай\n",
    "Будем считать, что существуют такие параметры $w_*$ и $b_*$,\n",
    "что соответствующий им классификатор $a(x)$ не допускает ни одной ошибки\n",
    "на обучающей выборке.\n",
    "В этом случае говорят, что выборка __линейно разделима__.\n",
    "\n",
    "Пусть задан некоторый классификатор $a(x) = sign (\\langle w, x \\rangle + b)$.\n",
    "Заметим, что если одновременно умножить параметры $w$ и $b$\n",
    "на одну и ту же положительную константу,\n",
    "то классификатор не изменится.\n",
    "Распорядимся этой свободой выбора и отнормируем параметры так, что\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:svmNormCond}\n",
    "    \\min_{x \\in X} | \\langle w, x \\rangle + b| = 1.\n",
    "\\end{equation}\n",
    "\n",
    "Можно показать, что расстояние от произвольной точки $x_0 \\in R^d$ до гиперплоскости,\n",
    "определяемой данным классификатором, равно\n",
    "\n",
    "$$\n",
    "    \\rho(x_0, a)\n",
    "    =\n",
    "    \\frac{\n",
    "        |\\langle w, x \\rangle + b|\n",
    "    }{\n",
    "        \\|w\\|\n",
    "    }.\n",
    "$$\n",
    "\n",
    "Тогда расстояние от гиперплоскости до ближайшего объекта обучающей выборки равно\n",
    "\n",
    "$$\n",
    "    \\min_{x \\in X}\n",
    "    \\frac{\n",
    "        |\\langle w, x \\rangle + b|\n",
    "    }{\n",
    "        \\|w\\|\n",
    "    }\n",
    "    =\n",
    "    \\frac{1}{\\|w\\|} \\min_{x \\in X} |\\langle w, x \\rangle + b|\n",
    "    =\n",
    "    \\frac{1}{\\|w\\|}.\n",
    "$$\n",
    "\n",
    "Данная величина также называется __отступом (margin)__.\n",
    "\n",
    "Таким образом, если классификатор без ошибок разделяет обучающую выборку,\n",
    "то ширина его разделяющей полосы равна $\\frac{2}{\\|w\\|}$.\n",
    "Известно, что максимизация ширины разделяющей полосы приводит\n",
    "к повышению обобщающей способности классификатора.\n",
    "Вспомним также, что на повышение обобщающей способности направлена и регуляризация,\n",
    "которая штрафует большую норму весов -- а чем больше норма весов,\n",
    "тем меньше ширина разделяющей полосы.\n",
    "\n",
    "Итак, требуется построить классификатор, идеально разделяющий обучающую выборку,\n",
    "и при этом имеющий максимальный отступ.\n",
    "Запишем соответствующую оптимизационную задачу,\n",
    "которая и будет определять метод опорных векторов для линейно разделимой выборки (hard margin support vector machine):\n",
    "\\begin{equation}\n",
    "\\label{eq:svmSep}\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & \\frac{1}{2} \\|w\\|^2 \\to \\min_{w, b} \\\\\n",
    "            & y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right) \\geq 1, \\quad i = 1, \\dots, \\ell.\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Неразделимый случай\n",
    "Рассмотрим теперь общий случай, когда выборку\n",
    "невозможно идеально разделить гиперплоскостью.\n",
    "Это означает, что какие бы $w$ и $b$ мы не взяли,\n",
    "хотя бы одно из ограничений в предыдущей задаче будет нарушено:\n",
    "\n",
    "$$\n",
    "    \\exists x_i \\in X:\\\n",
    "    y_i \\left(\n",
    "        \\langle w, x_i \\rangle + b\n",
    "    \\right) < 1.\n",
    "$$\n",
    "\n",
    "Сделаем эти ограничения \"мягкими\", введя штраф $\\xi_i \\geq 0$ за их нарушение:\n",
    "\n",
    "$$\n",
    "    y_i \\left(\n",
    "        \\langle w, x_i \\rangle + b\n",
    "    \\right) \\geq 1 - \\xi_i, \\quad i = 1, \\dots, \\ell.\n",
    "$$\n",
    "\n",
    "Отметим, что если отступ объекта лежит между нулем и\n",
    "единицей ($0 \\leq y_i \\left( \\langle w, x_i \\rangle + b \\right) < 1$),\n",
    "то объект верно классифицируется, но имеет ненулевой штраф $\\xi > 0$.\n",
    "Таким образом, мы штрафуем объекты за попадание внутрь разделяющей полосы.\n",
    "\n",
    "Величина $\\frac{1}{\\|w\\|}$ в данном случае называется мягким отступом (soft margin).\n",
    "С одной стороны, мы хотим максимизировать отступ, с другой -- минимизировать\n",
    "штраф за неидеальное разделение выборки $\\sum_{i = 1}^{\\ell} \\xi_i$.\n",
    "Эти две задачи противоречат друг другу: как правило, излишняя подгонка под\n",
    "выборку приводит к маленькому отступу, и наоборот -- максимизация отступа\n",
    "приводит к большой ошибке на обучении.\n",
    "В качестве компромисса будем минимизировать взвешенную сумму двух указанных величин.\n",
    "Приходим к оптимизационной задаче,\n",
    "соответствующей методу опорных векторов для линейно неразделимой выборки (soft margin support vector machine)\n",
    "\\begin{equation}\n",
    "\\label{eq:svmUnsep}\n",
    "    \\left\\{\n",
    "        \\begin{aligned}\n",
    "            & \\frac{1}{2} \\|w\\|^2 + C \\sum_{i = 1}^{\\ell} \\xi_i \\to \\min_{w, b, \\xi} \\\\\n",
    "            & y_i \\left(\n",
    "                \\langle w, x_i \\rangle + b\n",
    "            \\right) \\geq 1 - \\xi_i, \\quad i = 1, \\dots, \\ell, \\\\\n",
    "            & \\xi_i \\geq 0, \\quad i = 1, \\dots, \\ell.\n",
    "        \\end{aligned}\n",
    "    \\right.\n",
    "\\end{equation}\n",
    "Чем больше здесь параметр $C$, тем сильнее мы будем настраиваться на обучающую выборку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исследуем зависимость положения разделяющей гиперплоскости в методе опорных векторов в зависимости от значения гиперпараметра $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_size=500\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "mean0 = [7, 5]\n",
    "cov0 = [[4, 0], [0, 1]]  # diagonal covariance\n",
    "mean1 = [0, 0]\n",
    "cov1 = [[4, 0], [0, 2]]\n",
    "data0 = np.random.multivariate_normal(mean0, cov0, class_size)\n",
    "data1 = np.random.multivariate_normal(mean1, cov1, class_size)\n",
    "data = np.vstack((data0, data1))\n",
    "y = np.hstack((-np.ones(class_size), np.ones(class_size)))\n",
    "\n",
    "plt.scatter(data0[:, 0], data0[:, 1], c='red', s=50)\n",
    "plt.scatter(data1[:, 0], data1[:, 1], c='green', s=50)\n",
    "plt.legend(['y = -1', 'y = 1'])\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-5,15])\n",
    "axes.set_ylim([-5,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM_classifier = SVC(C=0.01, kernel='linear') # changing C here\n",
    "SVM_classifier.fit(data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = SVM_classifier.coef_[0][0]\n",
    "w_2 = SVM_classifier.coef_[0][1]\n",
    "w_0 = SVM_classifier.intercept_[0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(data0[:, 0], data0[:, 1], c='red', s=50)\n",
    "plt.scatter(data1[:, 0], data1[:, 1], c='green', s=50)\n",
    "plt.legend(['y = -1', 'y = 1'])\n",
    "x_arr = np.linspace(-10, 15, 3000)\n",
    "plt.plot(x_arr, -(w_0 + w_1 * x_arr) / w_2)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-5,15])\n",
    "axes.set_ylim([-5,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(data0[:, 0], data0[:, 1], c='red', s=50, label='y = -1')\n",
    "plt.scatter(data1[:, 0], data1[:, 1], c='green', s=50, label='y = +1')\n",
    "#plt.legend(['y = -1', 'y = 1'])\n",
    "x_arr = np.linspace(-10, 15, 3000)\n",
    "colors = ['red', 'orange', 'green', 'blue', 'magenta']\n",
    "\n",
    "for i, C in enumerate([0.0001, 0.01,  1, 100, 10000]):\n",
    "    SVM_classifier = SVC(C=C, kernel='linear')\n",
    "    SVM_classifier.fit(data, y)\n",
    "    w_1 = SVM_classifier.coef_[0][0]\n",
    "    w_2 = SVM_classifier.coef_[0][1]\n",
    "    w_0 = SVM_classifier.intercept_[0]\n",
    "    plt.plot(x_arr, -(w_0 + w_1 * x_arr) / w_2, color=colors[i], label='C='+str(C))\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-5,15])\n",
    "axes.set_ylim([-5,10])\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметр $C$ отвечает за то, что является более приоритетным для классификатора, — \"подгонка\" под обучающую выборку или максимизация ширины разделяющей полосы.\n",
    " - При больших значениях $C$ классификатор сильно настраивается на обучение, тем самым сужая разделяющую полосу.\n",
    " - При маленьких значениях $C$ классификатор расширяет разделяющую полосу, при этом допуская ошибки на некоторых объектах обучающей выборки."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
